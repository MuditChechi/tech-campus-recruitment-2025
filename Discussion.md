During the development of this solution, multiple approaches were explored to handle a 1 TB log file efficiently:

1Ô∏è‚É£ Na√Øve Line-by-Line Reading (Slow ‚ùå)

Used a simple loop to read the file line by line.
Issue: Extremely slow (30+ minutes for large files) due to high disk I/O overhead.
2Ô∏è‚É£ Buffered I/O with Chunk Reading (Faster ‚úÖ)

Instead of reading one line at a time, read 1MB chunks to reduce I/O overhead.
Benefit: Improved performance (50-60 seconds execution time).
3Ô∏è‚É£ Memory-Mapped File (mmap) (Didn‚Äôt Work as Expected ‚ùå)

Used mmap to read files without loading into memory.
Issue: Surprisingly slower (~61s execution time) due to inefficient line splitting.
4Ô∏è‚É£ Buffered I/O + Multi-threading (Final Solution ‚úÖ)

Combined 1MB buffered reading with multi-threading (8 workers).
Benefit: Achieved the best performance (25 seconds execution time).
5Ô∏è‚É£ System-Level Optimization (grep) (Best for Linux ‚úÖ)

Running grep on Linux/macOS was the fastest solution (1-5 seconds).
Issue: Cannot be used if Python-only solutions are required.
üìå Final Solution Summary
The final solution (Buffered I/O + Multi-threading) was chosen because:
‚úÖ Fastest Python-based implementation (25s execution time).
‚úÖ Memory-efficient (Processes 1MB chunks at a time).
‚úÖ Utilizes parallel processing (8 threads for faster extraction).
‚úÖ Scales well for 1 TB files without high RAM usage.